{"version":"7","defaultTimeframe":{"from":"now()-2h","to":"now()"},"defaultSegments":[],"sections":[{"id":"ba93c209-28d0-48aa-a1af-da671e399f93","type":"markdown","markdown":"# OPMIG-06: Processing, Parsing & Transformation\n\n> **Series:** OPMIG | **Notebook:** 6 of 9 | **Created:** December 2025\n\n> **OpenPipeline Migration Series** | Notebook 6 of 9  \n> **Level:** Intermediate to Advanced  \n> **Estimated Time:** 75 minutes\n\n---\n\n## Learning Objectives\n\nBy completing this notebook, you will:\n\n1. Master Dynatrace Pattern Language (DPL) for log parsing\n2. Configure DQL processors for data transformation\n3. ‚≠ê **NEW:** Parse Apache, Nginx, and JSON logs with production-ready patterns\n4. ‚≠ê **NEW:** Migrate from ELK/Logstash (Grok to DPL conversion)\n5. ‚≠ê **NEW:** Use the complete Parsing Pattern Library (timestamps, stack traces, HTTP)\n6. Implement drop processors for cost optimization\n7. Validate parsing success rates and troubleshoot failures\n\n---\n"},{"id":"0465fc6a-9afb-4366-806b-a62761e80146","type":"markdown","markdown":"---\n\n## Processing Stage Overview\n\nThe Processing stage is where data transformation happens. It includes three sub-stages executed in order:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    PROCESSING STAGE                              ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                  ‚îÇ\n‚îÇ  1. MASKING (Security First)                                     ‚îÇ\n‚îÇ     ‚îî‚îÄ Redact sensitive data before any other processing         ‚îÇ\n‚îÇ                                                                  ‚îÇ\n‚îÇ  2. FILTERING (Drop)                                             ‚îÇ\n‚îÇ     ‚îî‚îÄ Remove unwanted records before further processing         ‚îÇ\n‚îÇ                                                                  ‚îÇ\n‚îÇ  3. PROCESSING (Transform)                                       ‚îÇ\n‚îÇ     ‚îú‚îÄ DQL Processors (fieldsAdd, parse, etc.)                  ‚îÇ\n‚îÇ     ‚îú‚îÄ Technology Parsers (JSON, Apache, etc.)                   ‚îÇ\n‚îÇ     ‚îî‚îÄ Custom transformations                                    ‚îÇ\n‚îÇ                                                                  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Processor Execution Order\n\nWithin each sub-stage, processors execute in the order they're defined. You can reorder them in the UI.\n\n> üí° **Best Practice:** Order processors logically - parse first, then enrich with computed fields based on parsed values."},{"id":"a75af2af-7a7a-4636-bbb1-856bc56c02d5","type":"markdown","markdown":"---\n\n## DQL Processor Commands\n\nThe DQL processor supports a subset of DQL commands for data transformation.\n\n### Available Commands\n\n| Command | Purpose | Example |\n|---------|---------|----------|\n| `fieldsAdd` | Add new fields | `fieldsAdd env = \"production\"` |\n| `fieldsRemove` | Remove fields | `fieldsRemove sensitive_field` |\n| `fieldsRename` | Rename fields | `fieldsRename old = new_name` |\n| `parse` | Extract with DPL | `parse content, \"INT:count\"` |\n\n### fieldsAdd Examples\n\n#### Static Values\n```dql\nfieldsAdd environment = \"production\"\nfieldsAdd application = \"checkout-service\"\nfieldsAdd team = \"platform-engineering\"\n```\n\n#### Conditional Values (if/else)\n```dql\nfieldsAdd severity = if(loglevel == \"ERROR\", \"critical\",\n                     else: if(loglevel == \"WARN\", \"warning\",\n                     else: \"info\"))\n```\n\n#### Computed Values\n```dql\nfieldsAdd message_length = stringLength(content)\nfieldsAdd short_host = substring(host.name, 0, 15)\nfieldsAdd is_error = loglevel == \"ERROR\"\n```\n\n#### String Operations\n```dql\nfieldsAdd normalized_status = toLowerCase(status)\nfieldsAdd log_prefix = substring(content, 0, 50)\nfieldsAdd clean_message = trim(content)\n```\n\n#### Coalesce (First Non-Null)\n```dql\nfieldsAdd effective_level = coalesce(loglevel, status, \"UNKNOWN\")\n```\n\n### fieldsRemove Examples\n\n```dql\n// Remove single field\nfieldsRemove internal_id\n\n// Remove multiple fields\nfieldsRemove temp_field, debug_info, internal_state\n```\n\n### fieldsRename Examples\n\n```dql\n// Standardize field names\nfieldsRename user_id = userId\nfieldsRename request_id = requestId\nfieldsRename transaction_id = transactionId\n```"},{"id":"1b7a9e86-5dd6-4a94-a408-a0bb75b72401","type":"markdown","markdown":"---\n\n## Dynatrace Pattern Language (DPL)\n\nDPL is a powerful pattern matching language for extracting structured data from text.\n\n### Core Matchers\n\n| Matcher | Description | Matches |\n|---------|-------------|----------|\n| `INT` | Integer | `42`, `-17`, `0` |\n| `LONG` | Long integer | `1234567890123` |\n| `DOUBLE` | Decimal | `3.14`, `-0.5`, `1.0` |\n| `IPADDR` | IP address | `192.168.1.1`, `::1` |\n| `IPV4ADDR` | IPv4 only | `10.0.0.1` |\n| `IPV6ADDR` | IPv6 only | `2001:db8::1` |\n| `LD` | Line data (to delimiter) | Any text until next match |\n| `DATA` | Greedy match | Everything remaining |\n| `SPACE` | Whitespace | Spaces, tabs |\n| `NSPACE` | Non-whitespace | Word-like content |\n| `WORD` | Word chars | `hello`, `user123` |\n| `JSON` | JSON object | `{\"key\": \"value\"}` |\n| `EOL` | End of line | Line terminator |\n\n### Pattern Syntax Elements\n\n| Syntax | Meaning | Example |\n|--------|---------|----------|\n| `MATCHER:field` | Extract to named field | `INT:count` |\n| `MATCHER` | Match but don't extract | `SPACE` |\n| `MATCHER?` | Optional match | `(':' INT:port)?` |\n| `'literal'` | Match exact text | `'error_code='` |\n| `(a\\|b)` | Alternatives | `('user='\\|'userId=')` |\n| `MATCHER{n,m}` | Quantifier | `WORD{1,3}` |\n\n### Basic Parse Examples\n\n```dql\n// Extract user ID after \"user=\"\nparse content, \"'user=' LD:user_id\"\n\n// Extract error code (integer)\nparse content, \"'error_code=' INT:error_code\"\n\n// Extract IP and port\nparse content, \"IPADDR:client_ip ':' INT:port\"\n\n// Extract JSON payload\nparse content, \"LD JSON:payload\"\n```"},{"id":"f45c891b-ca52-4989-9bf2-719085708282","type":"markdown","markdown":"---\n\n## Real-World Log Format Examples ‚≠ê NEW\n\nProduction-ready DPL patterns for the most common log formats.\n\n### Apache Access Logs (Common Log Format)\n\n**Sample:** `192.168.1.100 - frank [12/Dec/2024:10:30:45 +0000] \"GET /api/users HTTP/1.1\" 200 1234`\n\n```dql\nparse content, \"\"\"\n  IPADDR:client_ip SPACE '-' SPACE LD:user SPACE\n  '[' TIMESTAMP('dd/MMM/yyyy:HH:mm:ss Z'):timestamp ']' SPACE\n  '\"' LD:method SPACE LD:request_path SPACE LD:protocol '\"' SPACE\n  INT:status_code SPACE INT:response_bytes\n\"\"\"\n```\n\n**Extracted:** client_ip, user, timestamp, method, request_path, protocol, status_code, response_bytes\n\n### Nginx (with Response Time)\n\n**Sample:** `192.168.1.50 - - [12/Dec/2024:10:30:45 +0000] \"POST /api/checkout HTTP/1.1\" 201 456 \"-\" \"Mozilla/5.0\" \"0.342\"`\n\n```dql\nparse content, \"\"\"\n  IPADDR:client_ip SPACE '-' SPACE '-' SPACE\n  '[' TIMESTAMP('dd/MMM/yyyy:HH:mm:ss Z'):timestamp ']' SPACE\n  '\"' LD:method SPACE LD:request_path SPACE LD:protocol '\"' SPACE\n  INT:status_code SPACE INT:response_bytes SPACE\n  '\"' LD:referrer '\"' SPACE '\"' LD:user_agent '\"' SPACE\n  '\"' DOUBLE:response_time_sec '\"'\n\"\"\"\n| fieldsAdd response_time_ms = toInt(response_time_sec * 1000)\n```\n\n### JSON Application Logs\n\n**Sample:** `{\"timestamp\":\"2024-12-12T10:30:45.123Z\",\"level\":\"ERROR\",\"service\":\"payment-api\",\"message\":\"Payment gateway timeout\"}`\n\n**Option 1: Use Technology Parser** (Recommended)\n- Add **Technology** processor ‚Üí Select **JSON** parser\n- All fields automatically flattened\n\n**Option 2: DQL Parsing**\n```dql\nparse content, \"JSON:log_data\"\n| fieldsAdd service = log_data[\"service\"]\n| fieldsAdd message = log_data[\"message\"]\n| fieldsAdd level = log_data[\"level\"]\n```\n\n### Syslog (RFC 3164)\n\n**Sample:** `<34>Dec 12 10:30:45 webserver sshd[1234]: Failed password for admin from 192.168.1.100`\n\n```dql\nparse content, \"\"\"\n  '<' INT:priority '>' \n  TIMESTAMP('MMM dd HH:mm:ss'):timestamp SPACE\n  LD:hostname SPACE LD:app_name '[' INT:pid ']:' SPACE\n  DATA:message\n\"\"\"\n| fieldsAdd facility = toInt(priority / 8)\n| fieldsAdd severity = toInt(priority % 8)\n```\n\n### Java Application Logs (Log4j)\n\n**Sample:** `2024-12-12 10:30:45,123 [http-nio-8080-exec-5] ERROR com.example.Service - Payment failed`\n\n```dql\nparse content, \"\"\"\n  TIMESTAMP('yyyy-MM-dd HH:mm:ss,SSS'):log_timestamp SPACE\n  '[' LD:thread ']' SPACE\n  LD:level SPACE LD:logger SPACE '-' SPACE\n  DATA:message\n\"\"\"\n```\n\n**Stack Trace Extraction:**\n```dql\nparse content, \"LD:exception_class ':' SPACE LD:exception_message EOL\"\n| parse content, \"'at ' LD:error_location '(' LD:file ':' INT:line_number ')'\"\n```\n\n### Kubernetes / Container Logs\n\n**Sample:** `2024-12-12T10:30:45.123456789Z stdout F {\"level\":\"info\",\"msg\":\"Request processed\"}`\n\n```dql\nparse content, \"\"\"\n  TIMESTAMP('yyyy-MM-dd\\'T\\'HH:mm:ss.SSSSSSSSSXXX'):k8s_timestamp SPACE\n  LD:stream SPACE LD:log_tag SPACE\n  JSON:log_payload\n\"\"\"\n| fieldsAdd level = log_payload[\"level\"]\n| fieldsAdd msg = log_payload[\"msg\"]\n```\n\n---\n"},{"id":"c90ca57a-6d47-4114-be36-aefae403b82c","type":"markdown","markdown":"---\n\n## Common Parsing Patterns\n\n### Apache/Nginx Access Logs\n\n**Sample log:**\n```\n192.168.1.100 - - [12/Dec/2024:10:30:45 +0000] \"GET /api/users HTTP/1.1\" 200 1234\n```\n\n**DPL Pattern:**\n```dql\nparse content, \"IPADDR:client_ip SPACE '-' SPACE LD:user SPACE '[' LD:timestamp ']' SPACE '\\\"' LD:method SPACE LD:path SPACE LD:protocol '\\\"' SPACE INT:status_code SPACE INT:bytes\"\n```\n\n### Key-Value Logs\n\n**Sample log:**\n```\nuserId=12345, action=login, status=success, duration=150ms\n```\n\n**DPL Patterns:**\n```dql\n// Extract each key-value pair\nparse content, \"'userId=' INT:user_id\"\nparse content, \"'action=' LD:action ','\"\nparse content, \"'status=' LD:status ','\"\nparse content, \"'duration=' INT:duration_ms 'ms'\"\n```\n\n### Flexible User ID Extraction\n\n**Sample logs with varying formats:**\n```\nProcessing request for user=john123\nUser userId=john123 authenticated\nRequest from user_id=john123 received\n```\n\n**DPL Pattern (alternatives):**\n```dql\nparse content, \"('user='|'userId='|'user_id=') LD:user_id\"\n```\n\n### Timestamp Parsing\n\n**DPL with timestamp format:**\n```dql\nparse content, \"TIMESTAMP('yyyy-MM-dd HH:mm:ss'):log_timestamp\"\nparse content, \"TIMESTAMP('dd/MMM/yyyy:HH:mm:ss Z'):apache_time\"\n```\n\n### Optional Fields\n\n**Sample log:**\n```\nRequest to server:8080 completed\nRequest to server completed\n```\n\n**DPL Pattern (optional port):**\n```dql\nparse content, \"'Request to ' LD:server (':' INT:port)? ' completed'\"\n```\n\n### Stack Trace Extraction\n\n**DPL Pattern:**\n```dql\nparse content, \"LD:exception_class ':' LD:exception_message\"\n```"},{"id":"d5cbf230-b265-47ae-8975-5480b2ebec6d","type":"markdown","markdown":"---\n\n## Data Transformation Examples\n\n### Example 1: Parse and Enrich Application Logs\n\n**Input:**\n```json\n{\"content\": \"[2024-12-12T10:30:45] ERROR PaymentService - Payment failed for orderId=12345, amount=99.99\"}\n```\n\n**Processor 1: Parse log structure**\n```dql\nparse content, \"'[' TIMESTAMP('yyyy-MM-dd\\'T\\'HH:mm:ss'):log_time ']' SPACE LD:level SPACE LD:service ' - ' DATA:message\"\n```\n\n**Processor 2: Extract payment details**\n```dql\nparse content, \"'orderId=' INT:order_id ','\"\n| parse content, \"'amount=' DOUBLE:amount\"\n```\n\n**Processor 3: Add computed fields**\n```dql\nfieldsAdd severity = if(level == \"ERROR\", \"critical\", else: \"normal\")\n| fieldsAdd application_tier = \"payment\"\n```\n\n### Example 2: JSON Log Processing\n\n**Input:**\n```json\n{\"content\": \"{\\\"level\\\":\\\"info\\\",\\\"msg\\\":\\\"Request processed\\\",\\\"duration_ms\\\":150}\"}\n```\n\n**Processor: Parse JSON and flatten**\n```dql\nparse content, \"JSON:json_data\"\n```\n\nUsing a Technology Parser (JSON) is often easier for JSON logs.\n\n### Example 3: Multi-Format Log Normalization\n\n**Processor: Normalize different log level formats**\n```dql\nfieldsAdd normalized_level = if(contains(content, \"ERROR\") OR contains(content, \"error\"), \"ERROR\",\n                             else: if(contains(content, \"WARN\") OR contains(content, \"warn\"), \"WARN\",\n                             else: if(contains(content, \"INFO\") OR contains(content, \"info\"), \"INFO\",\n                             else: if(contains(content, \"DEBUG\") OR contains(content, \"debug\"), \"DEBUG\",\n                             else: \"UNKNOWN\"))))\n```"},{"id":"b88a2ac5-ce4c-41ac-85e8-557d24b0c8da","type":"markdown","markdown":"---\n\n## Parsing Pattern Library ‚≠ê NEW\n\n### Timestamp Patterns (10+ Formats)\n\n| Format | Example | DPL |\n|--------|---------|-----|\n| ISO 8601 | `2024-12-12T10:30:45Z` | `TIMESTAMP('yyyy-MM-dd\\'T\\'HH:mm:ssXXX')` |\n| ISO + MS | `2024-12-12T10:30:45.123Z` | `TIMESTAMP('yyyy-MM-dd\\'T\\'HH:mm:ss.SSSXXX')` |\n| Apache | `12/Dec/2024:10:30:45 +0000` | `TIMESTAMP('dd/MMM/yyyy:HH:mm:ss Z')` |\n| Syslog | `Dec 12 10:30:45` | `TIMESTAMP('MMM dd HH:mm:ss')` |\n| Java | `2024-12-12 10:30:45,123` | `TIMESTAMP('yyyy-MM-dd HH:mm:ss,SSS')` |\n| MySQL | `2024-12-12 10:30:45` | `TIMESTAMP('yyyy-MM-dd HH:mm:ss')` |\n| Unix Epoch | `1702380645` | `LONG:epoch` ‚Üí `toTimestamp(epoch * 1000)` |\n\n### HTTP Request Patterns\n\n```dql\n// Full request line\nparse content, \"'\"' LD:method SPACE LD:path SPACE LD:protocol '\"'\"\n\n// Separate path and query\nparse content, \"'\"' LD:method SPACE LD:path ('?' LD:query)? SPACE LD:protocol '\"'\"\n\n// RESTful API paths (/api/v1/users/12345)\nparse content, \"'/api/v' INT:api_version '/' LD:resource '/' INT:id\"\n```\n\n### Key-Value Patterns\n\n```dql\n// Simple: user=john count=42\nparse content, \"'user=' LD:user SPACE 'count=' INT:count\"\n\n// Quoted: user=\"John Doe\" email=\"john@example.com\"\nparse content, \"'user=\"' LD:user '\"' SPACE 'email=\"' LD:email '\"'\"\n\n// Logfmt: level=info msg=\"OK\" duration=150ms\nparse content, \"'level=' LD:level SPACE 'msg=\"' LD:msg '\"' SPACE 'duration=' INT:dur 'ms'\"\n```\n\n### Stack Trace Patterns\n\n```dql\n// Java exception\nparse content, \"LD:exception_class ':' SPACE LD:exception_message EOL\"\n\n// Stack trace line\nparse content, \"'at ' LD:class_method '(' LD:file ':' INT:line ')'\"\n\n// Caused by\nparse content, \"'Caused by: ' LD:caused_by ':' SPACE LD:message\"\n\n// Python traceback\nparse content, \"'File \"' LD:file '\", line ' INT:line ', in ' LD:function\"\n```\n\n### PII Masking Patterns\n\n```dql\n// Credit cards (1234-5678-9012-3456 ‚Üí ****-****-****-****)\nfieldsAdd content = replaceAll(content, \"\\\\b\\\\d{4}[\\\\s-]?\\\\d{4}[\\\\s-]?\\\\d{4}[\\\\s-]?\\\\d{4}\\\\b\", \"****-****-****-****\")\n\n// Partial masking (keep last 4)\nfieldsAdd content = replaceAll(content, \"\\\\b(\\\\d{4})[\\\\s-]?(\\\\d{4})[\\\\s-]?(\\\\d{4})[\\\\s-]?(\\\\d{4})\\\\b\", \"****-****-****-$4\")\n\n// Email addresses\nfieldsAdd content = replaceAll(content, \"\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\", \"***@***.***\")\n\n// SSN (123-45-6789 ‚Üí ***-**-****)\nfieldsAdd content = replaceAll(content, \"\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b\", \"***-**-****\")\n```\n\n### Network Patterns\n\n```dql\n// IP and port\nparse content, \"IPADDR:ip ':' INT:port\"\n\n// IPv4 only\nparse content, \"IPV4ADDR:ipv4\"\n\n// IPv6 only\nparse content, \"IPV6ADDR:ipv6\"\n\n// URL parsing\nparse content, \"LD:protocol '://' LD:hostname (':' INT:port)? LD:path ('?' LD:query)?\"\n```\n\n---\n"},{"id":"5a1eff32-405e-4fb4-8636-7fe2d32b0a76","type":"markdown","markdown":"---\n\n## ELK/Logstash Migration Patterns ‚≠ê NEW\n\n### Grok vs. DPL: Key Differences\n\n| Grok (Logstash) | DPL (OpenPipeline) |\n|----------------|-------------------|\n| `%{IP:client_ip}` | `IPADDR:client_ip` |\n| `%{INT:count}` | `INT:count` |\n| `%{WORD:user}` | `WORD:user` |\n| `%{GREEDYDATA:msg}` | `DATA:msg` |\n| `\\\\s+` | `SPACE` |\n\n### Apache Log Migration\n\n**Grok:** `%{COMBINEDAPACHELOG}`\n\n**DPL:**\n```dql\nparse content, \"\"\"\n  IPADDR:client_ip SPACE LD:ident SPACE LD:auth SPACE\n  '[' TIMESTAMP('dd/MMM/yyyy:HH:mm:ss Z'):timestamp ']' SPACE\n  '\"' LD:method SPACE LD:request_path (SPACE LD:http_version)? '\"' SPACE\n  INT:status_code SPACE (INT:response_bytes | '-') SPACE\n  '\"' LD:referrer '\"' SPACE '\"' LD:user_agent '\"'\n\"\"\"\n```\n\n### Logstash Filter ‚Üí OpenPipeline Mapping\n\n| Logstash Filter | OpenPipeline |\n|-----------------|-------------|\n| `grok { }` | DQL parse processor |\n| `mutate { add_field }` | `fieldsAdd field = \"value\"` |\n| `mutate { remove_field }` | `fieldsRemove field` |\n| `mutate { rename }` | `fieldsRename old = new` |\n| `drop { }` | Drop processor |\n| `json { }` | JSON Technology Parser |\n| `date { }` | TIMESTAMP in parse |\n\n### Complete Migration Example\n\n**Logstash:**\n```ruby\nfilter {\n  if [level] == \"DEBUG\" { drop { } }\n  grok { match => { \"message\" => \"%{TIMESTAMP_ISO8601:ts} %{LOGLEVEL:level} %{GREEDYDATA:msg}\" } }\n  mutate { add_field => { \"env\" => \"production\" } }\n}\n```\n\n**OpenPipeline:**\n1. Drop processor: `loglevel == \"DEBUG\"`\n2. DQL parse: `parse content, \"TIMESTAMP('yyyy-MM-dd HH:mm:ss'):ts SPACE LD:level SPACE DATA:msg\"`\n3. DQL add: `fieldsAdd env = \"production\"`\n\n---\n"},{"id":"282adc99-8c4e-46e6-b3e2-106cfb421143","type":"markdown","markdown":"---\n\n## Technology Bundle Parsers\n\nOpenPipeline includes built-in parsers for common log formats.\n\n### Available Technology Parsers\n\n| Parser | Log Format | Extracted Fields |\n|--------|------------|------------------|\n| **Apache** | Apache access logs | client_ip, method, path, status, bytes |\n| **Nginx** | Nginx access logs | Similar to Apache |\n| **JSON** | JSON-formatted logs | All JSON fields flattened |\n| **Syslog** | RFC 3164/5424 syslog | facility, severity, hostname, message |\n| **Log4j** | Java Log4j format | level, logger, thread, message |\n| **AWS CloudWatch** | AWS logs | AWS-specific fields |\n\n### When to Use Technology Parsers\n\n| Use Technology Parser | Use Custom DQL/DPL |\n|----------------------|--------------------|\n| Standard log format | Custom/proprietary format |\n| Quick setup needed | Specific field extraction |\n| Common technology | Conditional parsing |\n| All fields needed | Selective extraction |\n\n### Configuring Technology Parsers\n\n1. Open pipeline in OpenPipeline settings\n2. Go to **Processing** tab\n3. Click **+ Processor** ‚Üí **Technology**\n4. Select parser type\n5. Configure matching condition\n6. Save"},{"id":"8c324310-ac3c-437b-827a-2830a16c8b73","type":"markdown","markdown":"---\n\n## Drop Processors\n\nDrop processors remove records from the pipeline before storage.\n\n### Common Drop Patterns\n\n| Use Case | Matching Condition |\n|----------|--------------------|\n| Debug logs | `loglevel == \"DEBUG\"` |\n| Trace logs | `loglevel == \"TRACE\"` |\n| Health checks | `contains(content, \"health\")` |\n| Readiness probes | `contains(content, \"/ready\")` |\n| Metrics endpoints | `contains(content, \"/metrics\")` |\n| Heartbeats | `contains(content, \"heartbeat\")` |\n| Specific source | `log.source == \"noisy-service\"` |\n\n### Drop Processor Configuration\n\n```\nProcessor Type: Drop\nName: Drop debug logs\nMatching Condition: loglevel == \"DEBUG\" OR status == \"DEBUG\"\n```\n\n### Combining Drop Conditions\n\n```\n// Drop all non-essential logs\nloglevel == \"DEBUG\" \n  OR loglevel == \"TRACE\" \n  OR contains(content, \"health\") \n  OR contains(content, \"/metrics\")\n```\n\n> ‚ö†Ô∏è **Important:** Dropped data is gone forever. Test drop conditions carefully before deploying."},{"id":"e3164023-231e-4021-be27-d9d0fe9d1aa7","type":"markdown","markdown":"---\n\n## Advanced Processing Patterns\n\n### Pattern 1: Fix Missing Timestamp\n\nWhen logs have a custom timestamp format that's not recognized:\n\n```dql\n// Parse timestamp from content\nparse content, \"'[' TIMESTAMP('yyyy-MM-dd HH:mm:ss'):parsed_timestamp ']'\"\n```\n\n### Pattern 2: Fix Missing Log Level\n\nWhen logs don't have a standard loglevel field:\n\n```dql\n// Extract level from content\nfieldsAdd loglevel = if(contains(content, \"[ERROR]\") OR contains(content, \"ERROR:\"), \"ERROR\",\n                     else: if(contains(content, \"[WARN]\") OR contains(content, \"WARN:\"), \"WARN\",\n                     else: if(contains(content, \"[INFO]\") OR contains(content, \"INFO:\"), \"INFO\",\n                     else: if(contains(content, \"[DEBUG]\") OR contains(content, \"DEBUG:\"), \"DEBUG\",\n                     else: \"NONE\"))))\n```\n\n### Pattern 3: Parse JSON from Within Text\n\nWhen JSON is embedded in a log message:\n\n```dql\n// Extract JSON from text\nparse content, \"LD JSON:embedded_json LD\"\n```\n\n### Pattern 4: Compute Response Time Categories\n\n```dql\nfieldsAdd response_category = if(duration_ms < 100, \"fast\",\n                              else: if(duration_ms < 500, \"normal\",\n                              else: if(duration_ms < 1000, \"slow\",\n                              else: \"very_slow\")))\n```\n\n### Pattern 5: Standardize Boolean Fields\n\n```dql\nfieldsAdd is_success = if(status == \"success\" OR status == \"ok\" OR status == \"200\", true, else: false)\n```\n\n### Pattern 6: Extract Service Name from Path\n\n```dql\n// From /api/v1/users ‚Üí users\nparse content, \"'/api/v' INT '/' LD:service_name\"\n```"},{"id":"5fbca340-c76b-4b84-b4da-c1cdfbf0b7c8","type":"markdown","markdown":"---\n\n## Validating Your Processing\n\nAfter configuring processors, validate that parsing is working correctly."},{"id":"bcd713b8-258f-43cc-8fa8-60c5f525b953","type":"dql","filterSegments":[],"drilldownPath":[],"previousFilterSegments":[],"height":250,"state":{"input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"// Check parsing success rate across all pipelines\nfetch logs, from: now() - 1h\n| summarize {\ntotal = count(),\nwith_loglevel = countIf(isNotNull(loglevel)),\nwith_status = countIf(isNotNull(status)),\nwith_either = countIf(isNotNull(loglevel) OR isNotNull(status))\n}\n| fieldsAdd parsing_rate = round((toDouble(with_either) / toDouble(total)) * 100, decimals: 1)"},"visualizationSettings":{"table":{"hideColumnsForLargeResults":true},"autoSelectVisualization":true,"chartSettings":{}},"querySettings":{"maxResultRecords":1000,"defaultScanLimitGbytes":500,"maxResultMegaBytes":1,"defaultSamplingRatio":10,"enableSampling":false},"visualization":"table"}},{"id":"837b92da-e796-4276-9293-af7b02d5b15c","type":"dql","filterSegments":[],"drilldownPath":[],"previousFilterSegments":[],"height":250,"state":{"input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"// Parsing success rate by pipeline\nfetch logs, from: now() - 1h\n| filter isNotNull(dt.openpipeline.pipelines)\n| summarize {\ntotal = count(),\nparsed = countIf(isNotNull(loglevel))\n}, by: {dt.openpipeline.pipelines}\n| fieldsAdd parsing_rate = round((toDouble(parsed) / toDouble(total)) * 100, decimals: 1)\n| sort parsing_rate asc"},"visualizationSettings":{"table":{"hideColumnsForLargeResults":true},"autoSelectVisualization":true,"chartSettings":{}},"querySettings":{"maxResultRecords":1000,"defaultScanLimitGbytes":500,"maxResultMegaBytes":1,"defaultSamplingRatio":10,"enableSampling":false},"visualization":"table"}},{"id":"c0d1262a-a4fa-4b40-b15a-d7458be31d3c","type":"dql","filterSegments":[],"drilldownPath":[],"previousFilterSegments":[],"height":250,"state":{"input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"// Sample logs that failed parsing (no loglevel extracted)\nfetch logs, from: now() - 1h\n| filter isNull(loglevel) AND isNull(status)\n| fields timestamp, log.source, dt.openpipeline.pipelines, content\n| limit 25"},"visualizationSettings":{"table":{"hideColumnsForLargeResults":true},"autoSelectVisualization":true,"chartSettings":{}},"querySettings":{"maxResultRecords":1000,"defaultScanLimitGbytes":500,"maxResultMegaBytes":1,"defaultSamplingRatio":10,"enableSampling":false},"visualization":"table"}},{"id":"87bab727-d849-451c-aba3-04ae564d6362","type":"dql","filterSegments":[],"drilldownPath":[],"previousFilterSegments":[],"height":250,"state":{"input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"// Verify specific parsed fields exist\n// Replace 'your_field' with fields your parsing should create\nfetch logs, from: now() - 1h\n| filter isNotNull(dt.openpipeline.pipelines)\n| summarize {\ntotal = count(),\nwith_user_id = countIf(isNotNull(user_id)),\nwith_request_id = countIf(isNotNull(request_id))\n}, by: {dt.openpipeline.pipelines}"},"visualizationSettings":{"table":{"hideColumnsForLargeResults":true},"autoSelectVisualization":true,"chartSettings":{}},"querySettings":{"maxResultRecords":1000,"defaultScanLimitGbytes":500,"maxResultMegaBytes":1,"defaultSamplingRatio":10,"enableSampling":false},"visualization":"table"}},{"id":"7b12ad7e-bae9-44be-910a-d596a5fbac1c","type":"dql","filterSegments":[],"drilldownPath":[],"previousFilterSegments":[],"height":250,"state":{"input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"// Sample successfully parsed logs to verify field extraction\nfetch logs, from: now() - 1h\n| filter isNotNull(loglevel)\n| limit 20"},"visualizationSettings":{"table":{"hideColumnsForLargeResults":true},"autoSelectVisualization":true,"chartSettings":{}},"querySettings":{"maxResultRecords":1000,"defaultScanLimitGbytes":500,"maxResultMegaBytes":1,"defaultSamplingRatio":10,"enableSampling":false},"visualization":"table"}},{"id":"6a7f582a-ac04-4cdc-bb6b-4b50c82cfb68","type":"dql","filterSegments":[],"drilldownPath":[],"previousFilterSegments":[],"height":250,"state":{"input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"// Verify drop processors are working\n// If drops are configured, debug log count should be low\nfetch logs, from: now() - 1h\n| summarize {debug_count = countIf(loglevel == \"DEBUG\")}\n| fieldsAdd message = if(debug_count == 0, \"‚úÖ Drop processor working - no debug logs\",\nelse: \"‚ö†Ô∏è Debug logs still present - check drop configuration\")"},"visualizationSettings":{"table":{"hideColumnsForLargeResults":true},"autoSelectVisualization":true,"chartSettings":{}},"querySettings":{"maxResultRecords":1000,"defaultScanLimitGbytes":500,"maxResultMegaBytes":1,"defaultSamplingRatio":10,"enableSampling":false},"visualization":"table"}},{"id":"0850e27a-de63-494e-8534-543f26d92cd5","type":"dql","filterSegments":[],"drilldownPath":[],"previousFilterSegments":[],"height":250,"state":{"input":{"timeframe":{"from":"now()-2h","to":"now()"},"value":"// Check log level distribution after processing\nfetch logs, from: now() - 1h\n| summarize {log_count = count()}, by: {loglevel}\n| sort log_count desc"},"visualizationSettings":{"table":{"hideColumnsForLargeResults":true},"autoSelectVisualization":true,"chartSettings":{}},"querySettings":{"maxResultRecords":1000,"defaultScanLimitGbytes":500,"maxResultMegaBytes":1,"defaultSamplingRatio":10,"enableSampling":false},"visualization":"table"}},{"id":"61c8e36a-9941-4053-9d2b-c3e05dfeb6e4","type":"markdown","markdown":"---\n\n## DPL Architect Tool\n\nDynatrace provides a **DPL Architect** tool for building and testing patterns:\n\n### Accessing DPL Architect\n\n1. Navigate to **Settings ‚Üí OpenPipeline**\n2. When adding a parse processor, click **Open DPL Architect**\n3. Or access via: `https://{your-environment}.apps.dynatrace.com/ui/apps/dynatrace.dpl.architect`\n\n### Using DPL Architect\n\n1. Paste sample log content\n2. Build pattern interactively\n3. See extracted fields in real-time\n4. Copy pattern to processor definition\n\n> üí° **Tip:** Always test patterns in DPL Architect before deploying to production pipelines."},{"id":"0577652c-12b3-4a24-a8bf-aa9fae8b9dee","type":"markdown","markdown":"---\n\n## Complete Processing Pipeline Example\n\n### Pipeline: `application-logs`\n\n**Processor 1: Drop Debug (Drop)**\n```\nMatching: loglevel == \"DEBUG\" OR contains(content, \"[DEBUG]\")\n```\n\n**Processor 2: Parse Application Log (DQL)**\n```dql\nparse content, \"'[' TIMESTAMP('yyyy-MM-dd HH:mm:ss'):log_ts ']' SPACE '[' LD:level ']' SPACE '[' LD:thread ']' SPACE LD:class ' - ' DATA:message\"\n```\n\n**Processor 3: Extract Request ID (DQL)**\n```dql\nparse content, \"'requestId=' LD:request_id\"\n```\n\n**Processor 4: Add Environment Tags (DQL)**\n```dql\nfieldsAdd environment = \"production\"\n| fieldsAdd application = \"checkout-service\"\n| fieldsAdd team = \"platform\"\n```\n\n**Processor 5: Compute Severity (DQL)**\n```dql\nfieldsAdd severity = if(level == \"ERROR\", \"P1\",\n                     else: if(level == \"WARN\", \"P2\",\n                     else: \"P3\"))\n```"},{"id":"f0f4bb10-1822-4e3d-bfbd-32b51aa10443","type":"markdown","markdown":"---\n\n## Next Steps\n\nNow that you can transform data, continue with:\n\n| Notebook | Focus Area |\n|----------|------------|\n| **OPMIG-07** | Metric & Event Extraction |\n| **OPMIG-08** | Security, Masking & Compliance |\n| **OPMIG-09** | Troubleshooting & Validation |\n\n---\n\n## References\n\n- [OpenPipeline Processing](https://docs.dynatrace.com/docs/discover-dynatrace/platform/openpipeline/concepts/processing)\n- [Processing Examples](https://docs.dynatrace.com/docs/discover-dynatrace/platform/openpipeline/use-cases/processing-examples)\n- [Dynatrace Pattern Language](https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/dynatrace-pattern-language)\n- [DPL Architect Tool](https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/dynatrace-pattern-language/dpl-architect)\n- [DQL Functions in OpenPipeline](https://docs.dynatrace.com/docs/discover-dynatrace/platform/openpipeline/reference/openpipeline-dql-functions)\n\n---\n\n*Last Updated: December 12, 2025*"}],"name":"--OPMIG-06-processing-parsing"}